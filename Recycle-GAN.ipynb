{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem\n",
    "- Suppose that we have two sets of data $X$ and $Y$ where each consists of ordered streams of datapoints $(x_1, x_2,\\ldots, x_t, \\ldots)$ and  $(y_1, y_2,\\ldots, y_s, \\ldots)$ for example consecutive frames from a video\n",
    "- The goal is to learn a mapping from the domain $X$ to the domain $Y$.\n",
    "- A naive approach would treat the set of sequences as set of the elements of all the sequences, disregarding the ordering of groups of elements.\n",
    "- For example we would treat a set of video clips as a set of the individual frames in the video clips\n",
    "- It turns out that taking advantage of the temporal ordering yields much better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "- Recurrent temporal predictor $P$:\n",
    "    - Takes as input the sequence $x_1,...,x_t$ and predicts the next element $x_{t+1}$ conditioned on the previous elements. \n",
    "    $$x_{t+1} = P_X(x_{1:t})$$\n",
    "    - Loss function\n",
    "        $$L_\\tau(P_X) = \\sum_t\\lVert x_{t+1} - P_X(x_{1:t})\\rVert^2$$\n",
    "- Based on this a recycle loss is defined\n",
    "TODO: loss formula\n",
    "- Here the predictor takes as input a sequence of generated samples $G_Y(x_1),...,G_Y(x_t)$ to predict the next one.\n",
    "- The generator $G_Y$ maps from $X$ to $Y$.\n",
    "- The predicted samples are then mapped back to $X$ via $G_X$ and the  loss between these and the elements of the original sequence is minimised.\n",
    "- The complete loss function:\n",
    "TODO: put the description under the term\n",
    "$$\\underset{\\text{generator loss for $G_X$}} {L_g(G_X, D_X)}\n",
    "+\\underset{\\text{generator loss for $G_Y$}}{L_g(G_Y, D_Y)}\\\\\n",
    "+ \\lambda_{rx}L_r(G_X, G_Y, P_Y) \\text{ }\\text{ }\\text{ }\\text{ }\\text{recycle loss for the mapping $Y \\longrightarrow X$}\\\\\n",
    "+ \\lambda_{ry}L_r(G_Y, G_X, P_X) \\text{ }\\text{ }\\text{ }\\text{ }\\text{recycle loss for the mapping $X \\longrightarrow Y$}\\\\\n",
    "+ \\lambda_{\\tau y}L_\\tau(P_X)\\text{ }\\text{ }\\text{ }\\text{ }\\text{recurrent loss for $X$}\\\\\n",
    "+ \\lambda_{\\tau x}L_\\tau(P_Y)\\text{ }\\text{ }\\text{ }\\text{ }\\text{recurrent loss for $Y$}\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sequences\n",
    "- A naive approach is to generate a video frame by frame where $y_t = G_Y(x_t)$.\n",
    "- However we could also incorporate temporal information by using $P_Y$ to smooth the output:\n",
    "\n",
    "    $$y_t = f(G_Y(x_t),P_Y(G_Y(x_{1:t-1})))$$ \n",
    "    \n",
    "- Here $f$ could be simple averaging: \n",
    "\n",
    "    $$y_t = \\frac{G_Y(x_t) + P_Y(G_Y(x_{1:t-1}))}{2}$$\n",
    "    \n",
    "- It could also be a non-linear function and possibly one that is learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation details\n",
    "- Spatial translation model uses CycleGAN\n",
    "- Temporal prediction model uses Pix2Pix\n",
    "- Discriminator is a $70 \\times 70$ PatchGAN\n",
    "- Same network architecture for $G_X$ and $G_Y$\n",
    "- Input size is $256 \\times 256$\n",
    "- Temporal predictors\n",
    "    - U-Net architecture\n",
    "    - Input is last two frames (does this mean P(x_{1:t}) = P(x_{t-2}, x_{t-1})$$\n",
    "- All the loss weights $\\lambda_s = 10$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
