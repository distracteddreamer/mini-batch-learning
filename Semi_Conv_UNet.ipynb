{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e150c602c919e4f0dadfd813a6b7628bbde2fdd"
      },
      "cell_type": "code",
      "source": "#TODO:\n# Data pipeline\n# Training graph\n# Loss function\n# Kmeans - structure properly \n# Mask generation\n# Metric",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e3dfdf7dfb93e7ea47c330b006d7675a0b92ebd2"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nimport os\nimport tensorflow as tf\nimport sys\nimport math",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0cd9518882ae36bef9f367a580906c7a07bb0ca7"
      },
      "cell_type": "markdown",
      "source": "PASCAL VOC colour map function which returns a colour map that enables us to match colours on the segmentation mask images with class labels."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea78d46ec9fdce856adb40068a91fa407b254635"
      },
      "cell_type": "code",
      "source": "def color_map(N=256, normalized=False):\n    def bitget(byteval, idx):\n        return ((byteval & (1 << idx)) != 0)\n\n    dtype = 'float32' if normalized else 'uint8'\n    cmap = np.zeros((N, 3), dtype=dtype)\n    for i in range(N):\n        r = g = b = 0\n        c = i\n        for j in range(8):\n            r = r | (bitget(c, 0) << 7-j)\n            g = g | (bitget(c, 1) << 7-j)\n            b = b | (bitget(c, 2) << 7-j)\n            c = c >> 3\n\n        cmap[i] = np.array([r, g, b])\n\n    cmap = cmap/255 if normalized else cmap\n    return cmap",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "A Config object that will store parameters that need to be passed around."
    },
    {
      "metadata": {
        "_uuid": "bc544cbfc3b0362070cef0ce5f92f92eeafcf870"
      },
      "cell_type": "markdown",
      "source": "A function to read an image, possibly downsample it and resize to specified dimensions with crop or pad (so as not to affect aspect ratio).\nTODO: add augmentations"
    },
    {
      "metadata": {
        "_uuid": "7b30f993b596a1a6fda7f8eccdf145c25babbf11"
      },
      "cell_type": "markdown",
      "source": "A function to read a mask. PASCAL VOC masks include a border around the segmentations for each object and pixels of predicted masks falling within the border might be excluded from calculations of losses and metrics. We"
    },
    {
      "metadata": {
        "_uuid": "a7f447455354604ebb82d2aefb6e698a9e3b93c3"
      },
      "cell_type": "markdown",
      "source": "We will dynamically obtain bounding boxes from the masks for now. Later we can run this once and save the results. It is necessary to match each instance to its class. The class colours are known beforehand. The documentation claims that the instance colours can be used to identify the class but does not specify how."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e02c63b9fabb9109beab942c9cac91dbd2feaa54"
      },
      "cell_type": "code",
      "source": "def get_class_mask(mask_rgb):\n    #Get one-hot masks for each class (excluding background and border)\n    masks = tf.reduce_all(tf.equal(mask_rgb[..., None, :], config.clrs_fgd[None, None]), \n                axis=-1) #(h, w, 1, 3)==(1, 1, n_classes, 3) -> (h, w, n_classes, 3) -> (h, w, n_classes)\n    return tf.to_float(masks)\n\ndef rgb2val(img):\n    img = tf.reduce_sum(img*tf.pow(256., [2,1,0])[None, None], axis=-1)\n    return img\n\ndef process_bboxes(bbox, config):\n    # bbox: batch_size x n_inst_max x 4\n    centre = (bbox[...,2:]+bbox[...,:2])/2\n    dims = (bbox[...,2:]-bbox[...,:2])\n    bbox = tf.concat([coords, dims], axis=-1)\n    return bbox\n\ndef get_bboxes_from_masks(masks):\n    # mask: h x w x n_inst\n    is_fgd = tf.greater(masks, 0)\n    \n    shape = tf.to_int64(tf.shape(masks))\n    \n    height = shape[0]\n    width = shape[1]\n    \n    horz_any = tf.to_float(tf.reduce_any(is_fgd, axis=0)) # h x n_inst\n    vert_any = tf.to_float(tf.reduce_any(is_fgd, axis=1)) # w x n_inst\n    \n    # Will find first non-zero column or row for each mask which will be \n    # coordinates top left corner of bbox\n    x1 = tf.argmax(horz_any, axis=0) # n_inst\n    y1 = tf.argmax(vert_any, axis=0) # n_inst\n    \n    # Reverse to find last non-zero column or row and subtract from the \n    # the corresponding mask dimensions - note that these are outside the bbox \n    x2 = width - tf.argmax(horz_any[::-1], axis=0) # n_inst\n    y2 = height - tf.argmax(vert_any[::-1], axis=0) # n_inst\n    \n    y2 = tf.where(tf.reduce_any(tf.greater(horz_any, 0), axis=0), y2, tf.zeros_like(y2)) # n_inst\n    x2 = tf.where(tf.reduce_any(tf.greater(vert_any, 0), axis=0), x2, tf.zeros_like(x2)) # n_inst\n    \n    boxes = tf.stack([y1, x1, y2, x2], axis=-1) # n_inst x 4\n    return tf.to_int32(boxes)\n    \n              \ndef get_masks_and_bboxes(class_mask_rgb, \n                         inst_mask_rgb, \n                         config):\n    #One-hot foreground class mask\n    class_mask_one_hot = get_class_mask(class_mask_rgb) #(h, w, n_classes)\n    \n    #Convert rgb to scalar values in order to identify unique instances,\n    #excluding background and border \n    inst_mask = rgb2val(inst_mask_rgb) # (h, w)\n    inst_vals_all = tf.unique(tf.reshape(inst_mask, [-1]))[0] # (n_inst,)\n    #Sort the unique values, exclude background and border (0, 255) so always first and last\n    #since these two are always present\n    inst_vals = tf.nn.top_k(inst_vals_all, k=tf.size(inst_vals_all))[0][1:-1]\n    #Select upto max_inst instances\n    inst_vals = tf.random_shuffle(inst_vals)[:config.max_inst] # n_inst_max = max(n_inst, max_inst)\n    \n    #One-hot instance mask which is then used to match each instance with its class\n    inst_mask_one_hot = tf.to_float(tf.equal(inst_mask[..., None], \n                                inst_vals[None, None]))# (h, w, 1)==(1,1,n_inst_max) -> (h, w, n_inst_max)\n    \n    if config.detection_mode == 'bbox':\n        paddings = [(0, config.max_inst - tf.shape(inst_vals)[0])]\n    \n        #Class mask for each instance\n        inst_class_mask = tf.argmax(class_mask_one_hot[...,None,:]*inst_mask_one_hot[...,None], axis=-1)\n                        # (h, w, 1, n_classes)*(h, w, n_inst_max, 1) -> (h, w, n_inst_max, n_classes) -> (h, w, n_inst_max)\n\n        #Get the target labels by taking the maximum of the class masks for each instance\n        #(which should have single non-zero value for each instance which is returned)\n        class_labels = tf.reduce_max(inst_class_mask, axis=(0, 1)) + 1 # (n_inst_max,) \n        class_labels = tf.pad(class_labels, paddings=paddings)\n\n        bboxes = get_bboxes_from_masks(inst_mask_one_hot)\n        bboxes = tf.pad(bboxes, paddings=paddings + [(0,0)])\n        \n        return tf.pad(inst_mask_one_hot, paddings=[(0,0), (0,0)] + paddings), class_labels, bboxes\n    \n    \n    elif config.detection_mode == 'semi_conv':\n        # Add a background channels as the first channel and then take argmax \n        class_mask_sparse = tf.argmax(tf.pad(class_mask_one_hot, [(0, 0), (0, 0), (1, 0)]), axis=-1)\n        inst_mask_sparse = tf.argmax(tf.pad(inst_mask_one_hot, [(0, 0), (0, 0), (1, 0)]), axis=-1)\n                      \n        #Where we restrict the number of instances, want to ensure that other instance are also masked\n        #from the class mask\n        class_mask_sparse = class_mask_sparse*tf.to_int64(tf.greater(inst_mask_sparse, 0))\n        \n        counts = tf.shape(inst_vals)[0] - 1 #should be in [0, max_inst)\n        return class_mask_sparse, inst_mask_sparse, counts\n    \n    \n    \n        \n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fffe84cb30e8b9b73d96a6c8ed91932cecde6c12"
      },
      "cell_type": "code",
      "source": "def read_img(img_file, img_type, config):\n    downsample = config.downsample\n    height = config.height\n    width = config.width\n    assert img_type in ['jpeg', 'png']\n    img_string = tf.read_file(img_file)\n    if img_type == 'jpeg':\n        img = tf.image.decode_jpeg(img_string)\n    if img_type == 'png':\n        img = tf.image.decode_png(img_string)\n    img = img[::downsample, ::downsample]\n    if height is not None and width is not None:\n        img = tf.image.resize_image_with_crop_or_pad(img, height, width)\n    img = tf.to_float(img)\n    img.set_shape([256, 256, 3])\n    return img\n\ndef read_masks(class_mask_file, inst_mask_file, config):\n    class_mask_rgb = read_img(class_mask_file, config.mask_type, config)\n    inst_mask_rgb = read_img(inst_mask_file, config.mask_type, config)\n    return class_mask_rgb, inst_mask_rgb\n\ndef read_data(img_file, class_mask_file, inst_mask_file, config):\n    img = read_img(img_file, config.img_type, config)\n    class_mask_rgb, inst_mask_rgb = read_masks(class_mask_file, inst_mask_file, config)\n    \n    #Normalize image to lie in [0,1]\n    img = img/255 \n    \n    #TODO: concatenate img and mask and apply augmentation, then split\n    \n    masks = get_masks_and_bboxes(class_mask_rgb, inst_mask_rgb, config)\n    return (img,) + masks ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "baeaa6c9edb65f9e86f6c02e5dc327e38a6a2afb"
      },
      "cell_type": "code",
      "source": "class Config(object):\n    def __init__(self):\n        self.height = 256\n        self.width = 256\n        self.downsample = 2\n        self.filespath = '../input/voctrainval_11-may-2012/VOCdevkit/VOC2012/ImageSets/Segmentation'\n        self.train = open(os.path.join(self.filespath, 'train.txt')).read().split('\\n')[:-1]\n        self.val = open(os.path.join(self.filespath, 'val.txt')).read().split('\\n')[:-1]\n        mid = len(self.val)//2\n        self.train =self.train + self.val[mid:]\n        self.val = self.val[:mid]\n        assert(len(set(self.train).intersection(set(self.val))) == 0)\n        self.imgs_path = '../input/voctrainval_11-may-2012/VOCdevkit/VOC2012/JPEGImages/{}.jpg'\n        self.class_segs_path = '../input/voctrainval_11-may-2012/VOCdevkit/VOC2012/SegmentationClass/{}.png'\n        self.inst_segs_path = '../input/voctrainval_11-may-2012/VOCdevkit/VOC2012/SegmentationObject/{}.png'\n        self.img_files, self.class_mask_files, self.inst_mask_files = \\\n            [[path.format(f) for f in self.train] for path in [self.imgs_path, self.class_segs_path, self.inst_segs_path]]\n        self.valid_img_files, self.valid_class_mask_files, self.valid_inst_mask_files = \\\n            [[path.format(f) for f in self.val] for path in [self.imgs_path, self.class_segs_path, self.inst_segs_path]]\n        self.n_epochs = 20\n        self.batch_size = 4\n        self.num_ex = len(self.img_files)\n        self.num_valid = len(self.valid_img_files)\n        self.batches_per_epoch = math.ceil(self.num_ex/self.batch_size)\n        self.num_valid_batches = math.ceil(self.num_valid/self.batch_size)\n        \n        \n        \n        self.img_type = 'jpeg'\n        self.mask_type = 'png'\n        \n        self.cmap = color_map()\n        self.labels = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', \n                      'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', \n                      'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', \n                      'sofa', 'train', 'tvmonitor', 'void']\n        \n        self.upsample = 'simple'\n        \n        self.n_classes = len(self.labels) - 1 #exclude void\n        \n        self.clrs = np.concatenate([self.cmap[:len(self.labels)-1], self.cmap[-1:]])\n        self.clrs_fgd = self.clrs[1:-1]\n        \n        self.max_inst = 20 #56\n        self.detection_mode = 'semi_conv'\n        \n        self.optimizer = 'AdamOptimizer'\n        self.optim_kwargs = {'learning_rate':1e-3}\n        \n        self.n_features = 8\n        \n        self.n_units = [1024, 1024]\n        self.max_ramp_iter = 80",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2771104707e8e87880cac017aee4e3dfd7ee8eb8"
      },
      "cell_type": "code",
      "source": "# def kmeans_fn(x, k, tol=1e-6, max_iters=10):\n#     # inputs: H*W x F\n#     assert (tol is not None) or (max_iters is not None)\n#     #vectors = tf.reshape(inputs, [-1, tf.shape(inputs)[-1]]) #[H*W, F]\n#     centres = tf.random.shuffle(x)[:k]#[k, F]\n#     iters = tf.constant(0)\n    \n#     def cluster_fn(clusters, mu, mu_prev, iters):\n#         sq_diff = (mu[:, None] - x[None])**2 # (k x 1 x F - 1 x H*W x F)**2 -> k x H*W x F\n#         dist = tf.reduce_sum(sq_diff, axis=-1) # k x H*W\n#         clusters = tf.argmin(dist, axis=0) # H*W \n#         clusters_one_hot = tf.to_float(tf.one_hot(clusters, depth=k, axis=0)) # k x H*W\n#         # k x H*W x 1 * 1 x H*W x F = k x H*W x F - > k x F\n#         # k x H*W -> k -> k x 1\n#         # k x F / k x 1 -> k x F\n#         mu_new = tf.reduce_sum(clusters_one_hot[..., None]*x[None], axis=1)/tf.reduce_sum(clusters_one_hot, axis=1)[:, None]\n#         return clusters, mu_new, mu, tf.add(iters, 1)\n    \n#     def cond(cl, m, mp, i):\n#         not_converged = tf.greater(tf.reduce_mean((m-mp)**2), tol) if tol is not None else tf.constant(True)\n#         not_converged = tf.logical_or(tf.equal(i, 0), not_converged)\n#         below_max_iters = tf.less(i, max_iters) if max_iters is not None else tf.constant(True)\n#         return tf.logical_and(not_converged, below_max_iters)\n    \n        \n        \n#     clusters, mu, mu_prev, n_iters  = tf.while_loop(cond=cond, body=cluster_fn, \n#                              loop_vars=[tf.zeros(tf.shape(x)[0], dtype=tf.int64), \n#                                        centres,\n#                                        centres, iters])\n    \n#     inertia = tf.reduce_sum((x - tf.gather(mu, clusters)) ** 2)\n    \n#     return clusters, mu, mu_prev, inertia, n_iters\n    \n# def kmeans_clustering(inputs, k, tol=1e-6, max_iters=10, n_init=10):\n#     clusters, mu, mu_prev, inertia, iters = tf.map_fn(elems=tf.range(n_init), \n#                                      fn=lambda _: kmeans_fn(inputs, k, tol, max_iters),\n#                                      dtype=(tf.int64, tf.float32, tf.float32, tf.float32, tf.int32))\n#     best = tf.argmin(inertia)\n#     return clusters[best], mu[best], mu_prev[best], iters[best]\n\n# def kmeans_layer(features, class_mask, k, tol=1e-6, max_iters=10, n_init=10):\n#     where = tf.where(tf.greater(class_mask, 0))\n#     vectors = tf.gather(features, where)\n#     clusters, mu, _, _ = kmeans_clustering(vectors, km tol, max_iters, n_init)\n#     instance_map = tf.scatter(clusters, where, tf.shape(class_mask))\n#     return instance_map, mu",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9587a6edbcd03cc7763d8eb00c0144377e06b99"
      },
      "cell_type": "code",
      "source": "# #TODO: get rid on pp_in_layers in appropriate\n\n# from tf.keras import models, layers\n# # Build U-Net model\ndef upsample_conv(inputs, **conv_kwargs):\n    return tf.layers.conv2d_transpose(inputs, **conv_kwargs)\n\ndef upsample_simple(inputs, **conv_kwargs):\n    return tf.image.resize_nearest_neighbor(inputs, \n                                            size=tf.multiply(tf.shape(inputs)[1:-1], conv_kwargs['strides']), \n                                            align_corners=True)\n\ndef conv_relu_bn(inputs, training, **conv_kwargs):\n    conv = tf.layers.conv2d(inputs, activation=tf.nn.relu, **conv_kwargs)\n    bn = tf.layers.batch_normalization(conv)\n    return bn\n\ndef dense_relu_bn(inputs, training, **dense_kwargs):\n    dense = tf.layers.dense(inputs, activation=tf.nn.relu, **dense_kwargs)\n    bn = tf.layers.batch_normalization(dense)\n    return bn\n\nupsample_fn = {'deconv':upsample_conv, 'simple':upsample_simple}\n    \n# # input_img = layers.Input(t_x.shape[1:], name = 'RGB_Input')\n# # pp_in_layer = input_img\n# # if NET_SCALING is not None:\n# #     pp_in_layer = layers.AvgPool2D(NET_SCALING)(pp_in_layer)\n    \n# # pp_in_layer = layers.GaussianNoise(GAUSSIAN_NOISE)(pp_in_layer)\n# # pp_in_layer = layers.BatchNormalization()(pp_in_layer)\n\ndef unet(inputs, training, n_classes, upsample):\n    conv1 = conv_relu_bn(inputs, training, filters=8, kernel_size=(3, 3), padding='same')\n    conv1 = conv_relu_bn(conv1, training, filters=8, kernel_size=(3, 3), padding='same')\n    pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2)\n\n    conv2 = conv_relu_bn(pool1, training, filters=16, kernel_size=(3, 3), padding='same')\n    conv2 = conv_relu_bn(conv2, training, filters=16, kernel_size=(3, 3), padding='same')\n    pool2 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n\n    conv3 = conv_relu_bn(pool2, training, filters=32, kernel_size=(3, 3), padding='same')\n    conv3 = conv_relu_bn(conv3, training, filters=32, kernel_size=(3, 3), padding='same')\n    pool3 = tf.layers.max_pooling2d(conv3, pool_size=2, strides=2)\n\n    conv4 = conv_relu_bn(pool3, training, filters=64, kernel_size=(3, 3), padding='same')\n    conv4 = conv_relu_bn(conv4, training, filters=64, kernel_size=(3, 3), padding='same')\n    pool4 = tf.layers.max_pooling2d(conv4, pool_size=2, strides=2)\n\n    conv5 = conv_relu_bn(pool4, training, filters=128, kernel_size=(3, 3), padding='same')\n    conv5 = conv_relu_bn(conv5, training, filters=128, kernel_size=(3, 3), padding='same')\n\n    up6 = upsample(conv5, filters=64, kernel_size=(2, 2), strides=(2, 2), padding='same')\n    up6 = tf.concat([up6, conv4], axis=-1)\n    conv6 = conv_relu_bn(up6, training, filters=64, kernel_size=(3, 3), padding='same')\n    conv6 = conv_relu_bn(conv6, training, filters=64, kernel_size=(3, 3), padding='same')\n\n    up7 = upsample(conv6, filters=32, kernel_size=(2, 2), strides=(2, 2), padding='same')\n    up7 = tf.concat([up7, conv3], axis=-1)\n    conv7 = conv_relu_bn(up7, training, filters=32, kernel_size=(3, 3), padding='same')\n    conv7 = conv_relu_bn(conv7, training, filters=32, kernel_size=(3, 3), padding='same')\n\n    up8 = upsample(conv7, filters=16, kernel_size=(2, 2), strides=(2, 2), padding='same') \n    up8 = tf.concat([up8, conv2], axis=-1)\n    conv8 = conv_relu_bn(up8, training, filters=16, kernel_size=(3, 3), padding='same')\n    conv8 = conv_relu_bn(conv8, training, filters=16, kernel_size=(3, 3), padding='same')\n\n    up9 = upsample(conv8, filters=8, kernel_size=(2, 2), strides=(2, 2), padding='same')\n    up9 = tf.concat([up9, conv1], axis=-1)\n    conv9 = conv_relu_bn(up9, training, filters=8, kernel_size=(3, 3), padding='same')\n    conv9 = conv_relu_bn(conv9, training, filters=8, kernel_size=(3, 3), padding='same')\n\n    class_map = tf.layers.conv2d(conv9, filters=n_classes, kernel_size=(1, 1))\n    \n    return conv9, class_map\n\ndef count_net(inputs, training, n_units, n_counts):\n    #conv = tf.layers.conv2d(inputs, kernel_size=(3, 3), filters=64)\n    flat = tf.layers.flatten(inputs)\n    dense1 = dense_relu_bn(flat, training, units=n_units[0])\n    dense2 = dense_relu_bn(dense1, training, units=n_units[1])\n    counts = dense_relu_bn(dense2, training, units=n_counts)\n    \n    return counts\n\ndef add_position_info_2d(inputs):\n    #inputs: (n, h, w, f)\n    shape = tf.shape(inputs)[1:-1]\n    delta = tf.meshgrid(tf.range(shape[0]), tf.range(shape[1]), indexing='ij') #[(h, w), (h, w)]\n    delta = tf.to_float(tf.stack(delta, axis=-1)/shape) #(h, w, 2)\n    inputs = tf.concat([inputs[...,:2] + delta[None], inputs[...,2:]], axis=-1)\n    return inputs\n\ndef semi_conv_layer(inputs, training, n_features):\n    semi_conv = tf.layers.conv2d(inputs, filters=n_features, kernel_size=(1, 1))\n    semi_conv = add_position_info_2d(semi_conv)\n    return semi_conv\n\ndef model(inputs, training, config):\n    feature_maps, class_map = unet(inputs, training, config.n_classes, upsample_fn[config.upsample])\n    semi_conv_inputs = tf.concat([feature_maps, class_map], axis=-1)\n    semi_conv = semi_conv_layer(semi_conv_inputs, training, config.n_features)\n    counts_pred = count_net(feature_maps, training, config.n_units, config.max_inst) \n    return class_map, semi_conv, counts_pred\n\ndef semi_conv_loss(y_true, y_pred):\n    \"\"\"\n    Implements equation 5 from https://arxiv.org/abs/1807.10712 for a mini-batch of images.\n    \n    Args:\n        y_true (Tensor): sparse label tensor of shape (batch_size x height x width), \n                         with a separate number for each instance present in the image.\n                         Requires that the values are consecutive integers starting from 0.\n        y_pred (Tensor): sparse prediction tensor of shape (batch_size x height x width x channels)\n        \n    Returns:\n        semi-convolutional loss \n    \"\"\"\n        #find the maximum number of instances in any image in this batch\n    n_inst_max = tf.to_int32(tf.reduce_max(y_true))\n    \n    #batch_size x height x width -> batch_size x n_inst_max x height x width\n    y_true_one_hot = tf.one_hot(y_true, depth=n_inst_max, axis=1)\n    \n    #results in tensor of shape batch_size x n_inst_max x height x width x channels\n    y_pred_dense = y_true_one_hot[...,tf.newaxis]*y_pred[:,tf.newaxis]\n    \n    #reshape to (batch_size*n_inst_max) x height x width x channels\n    y_pred_dense = tf.reshape(y_pred_dense, \n                              tf.concat([[-1], tf.shape(y_pred_dense)[2:]], axis=0))\n    \n    #batch_size x n_inst_max x height x width -> (batch_size*n_inst_max) x height x width\n    y_true_one_hot = tf.reshape(y_true_one_hot,\n                                tf.concat([[-1], tf.shape(y_true_one_hot)[2:]], axis=0))    \n        #find number of pixels in each instance\n    #(batch_size*n_inst_max) x height x width -> (batch_size*n_inst_max)\n    n_inst_pixels = tf.reduce_sum(y_true_one_hot, axis=[1, 2])\n    has_inst_mask = tf.greater(n_inst_pixels, 0)\n    \n    #num_unpadded x height x width x channels\n    y_pred_dense = tf.boolean_mask(y_pred_dense, has_inst_mask)\n    \n    #num_unpadded x height x width\n    y_true_one_hot = tf.boolean_mask(y_true_one_hot, has_inst_mask)\n    \n    #num_unpadded\n    n_inst_pixels = tf.boolean_mask(n_inst_pixels, has_inst_mask)\n    \n    #num_unpadded x height x width x channels -> num_unpadded x 1 x 1 x channels\n    embeds_sum = tf.reduce_sum(y_pred_dense, axis=[1,2], keep_dims=True)\n    #num_unpadded x height x width x channels -> num_unpadded x height x width\n    dist = tf.norm(y_pred_dense - embeds_sum/n_inst_pixels[:, None, None, None], axis=-1)\n    dist_masked = dist*y_true_one_hot\n    #num_unpadded x height x width -> num_unpadded\n    dist_avg = tf.reduce_sum(dist_masked, axis=[1,2])/n_inst_pixels\n    \n    #dist_avg = tf.Print(dist_avg, data=[dist_avg])\n    \n    #num_unpadded -> 1\n    loss = tf.reduce_sum(dist_avg)\n    \n    return loss\n    \n#     #(batch_size*n_inst_max) x height x width x channels -> (batch_size*n_inst_max) x channels\n#     embeds_sum = tf.reduce_sum(y_pred_dense, axis=[1,2], keep_dims=True)\n#     #(batch_size*n_inst_max) x height x width x channels -> (batch_size*n_inst_max) x height x width\n#     #dist = tf.norm(y_pred_dense*n_inst_pixels[:,None,None,None] - embeds_sum, axis=-1)\n    \n    \n#     #keep only the distances for pixels that belong to the instance\n#     dist_masked = dist*y_true_one_hot\n    \n#     #sum the losses for each instance\n#     #(batch_size*n_inst_max) x height x width -> (batch_size*n_inst_max)\n#     dist_sum = tf.reduce_sum(dist_masked, axis=[1,2])\n#     has_inst_mask = tf.greater(n_inst_pixels, 0)\n    \n#     #select only the elements of dist that correspond to an instance \n#     losses = (tf.boolean_mask(dist_sum, has_inst_mask)/\n#                     tf.boolean_mask(n_inst_pixels, has_inst_mask)**2)\n    \n#     loss = tf.reduce_sum(losses)\n    \n#     return loss, dist\n\ndef get_losses(class_pred, class_true, inst_maps, inst_true, counts_pred, counts_true, itr=None):\n    with tf.variable_scope('class_loss'):\n        class_loss = tf.losses.sparse_softmax_cross_entropy(logits=class_pred, labels=class_true)\n    with tf.variable_scope('inst_loss'):\n        inst_loss = semi_conv_loss(y_pred=inst_maps, y_true=inst_true)\n    with tf.variable_scope('count_loss'):\n        count_loss = tf.losses.sparse_softmax_cross_entropy(logits=counts_pred, labels=counts_true)\n        tf.add_to_collection('counts_pred', counts_pred)\n    if itr is not None:\n        T = tf.minimum(itr/float(config.max_ramp_iter), 1.)\n        count_weight = tf.exp(-5*(1-T)**2)\n        count_loss = count_weight*count_loss\n    losses = (class_loss, inst_loss, count_loss)\n    total_loss = tf.reduce_sum(losses)\n    return total_loss, losses\n\ndef get_train_op(loss, config):\n    optimizer = getattr(tf.train, config.optimizer)(**config.optim_kwargs)\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        train_step = optimizer.minimize(loss)\n    return train_step",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6fe8d2ef32e5e174542a7d749a088f8f091e27f"
      },
      "cell_type": "code",
      "source": "def create_data_pipeline(config, mode='train'):\n    file_lists = [config.img_files, config.class_mask_files, config.inst_mask_files]\n    file_datasets = tuple(map(tf.data.Dataset.from_tensor_slices, file_lists))\n    dataset = tf.data.Dataset.zip(file_datasets)\n    if mode=='train':\n        dataset = dataset.shuffle(len(config.img_files))\n    \n    dataset = dataset.map(lambda x, y, z: read_data(x, y, z, config))\n    dataset = dataset.batch(config.batch_size).repeat(config.n_epochs)\n    tf.add_to_collection('ITERATOR_{}'.format(mode.upper()),\n                         dataset.make_initializable_iterator())\n\ndef preproc(config):\n    dataset_handle = tf.placeholder(tf.string, shape=[])\n\n    for mode in ['train', 'valid']:\n        create_data_pipeline(config, mode)\n    \n    train_itr = tf.get_collection('ITERATOR_TRAIN')[0]\n    iterator = tf.data.Iterator.from_string_handle(dataset_handle, \n                                                  output_types=train_itr.output_types,\n                                                  output_shapes=train_itr.output_shapes)\n    return iterator.get_next(), dataset_handle",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b564aafd297503cc480870a4b3651b277db2f9b5"
      },
      "cell_type": "code",
      "source": "def init_iterators(sess):\n    for mode in ['train','valid']:\n        itr = tf.get_collection('ITERATOR_{}'.format(mode.upper()))[0]\n        sess.run(itr.initializer)\n        \ndef get_itr_handles(sess):\n    handles = []\n    for mode in ['train','valid']:\n        itr = tf.get_collection('ITERATOR_{}'.format(mode.upper()))[0]\n        handles.append(sess.run(itr.string_handle()))\n    return handles",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8619eab721dbf7f2771ce155a550437a816e8f80"
      },
      "cell_type": "code",
      "source": "def add_metric_avg_op(metric, step, name):\n    avg = tf.Variable(initial_value=0.)\n    avg = tf.assign(avg, (avg*(step-1) + metric)/step, name=name)\n    avg_reset = tf.assign(avg, 0)\n    return avg, avg_reset\n\ndef get_ema_var(graph, name):\n    return graph.get_tensor_by_name('{}/ExponentialMovingAverage:0'.format(name))\n\ntf.reset_default_graph()\n#config = train_utils.Config()\nconfig = Config()\n(images, class_masks, instance_masks, counts), dataset_handle = preproc(config)\n\n#model = getattr(import_module(config.model_module), config.model_function)\ntraining = tf.placeholder(shape=[], dtype=tf.bool, name='training')\ngraph = tf.get_default_graph()\nclass_pred, inst_maps, counts_pred = model(images, training, config)\n#itr = tf.placeholder(dtype=tf.float32, shape=[], name='itr')\ntotal_loss, losses = get_losses(class_pred, class_masks, inst_maps, instance_masks, counts_pred, counts)#, itr)\n\nstep = tf.assign_add(tf.Variable(initial_value=0.), 1)\nstep_reset = tf.assign(step, 0)\nloss_avg, loss_avg_reset = add_metric_avg_op(total_loss, step, 'loss_avg')\ncl_loss_avg, cl_loss_avg_reset = add_metric_avg_op(losses[0], step, 'cl_loss_avg')\ninst_loss_avg, inst_loss_avg_reset = add_metric_avg_op(losses[1], step, 'inst_loss_avg')\nct_loss_avg, ct_loss_avg_reset = add_metric_avg_op(losses[2], step, 'ct_loss_avg')\n\ntrain_op = get_train_op(total_loss, config)\n# ema = tf.train.ExponentialMovingAverage(config.ema_decay)\n# ema_op = ema.apply([loss, dice])\n# tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, ema_op)\n\n# loss_ema, dice_ema = [get_ema_var(graph, metric) for metric in ['loss', 'dice']]\n\n# Note that we display the running average metrics but tensorboard gets the raw ones\n# train_summary_op = add_scalar_summaries({'loss': loss, 'dice': dice})\n# valid_summary_op = add_scalar_summaries({'loss': loss_avg, 'dice': dice_avg}, postfix='val')\n# img_summary_op = add_img_summary(images, masks, probs)\n                                  \n# update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n# with tf.control_dependencies(update_ops):\n#     optim = getattr(tf.train, config.optimizer)(config.lr)\n#     train_step = optim.minimize(loss)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6943a7d561b966a39898148e0df81ff6d3b9f879"
      },
      "cell_type": "code",
      "source": "steps_per_epoch = np.ceil(len(config.train)/config.batch_size).astype('int')\nmax_iters = steps_per_epoch*config.n_epochs\nnum_valid_steps = np.ceil(len(config.val)/config.batch_size).astype('int')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d641c37a36e57fca4c744a9a97843e31b29b02a3"
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    init_iterators(sess)\n    \n    train_handle, valid_handle = get_itr_handles(sess)\n    \n    for it in range(1, max_iters+1):\n        #tt = np.float32(it//100 + 1.)\n        _, tl, ls, count_val = sess.run([train_op, loss_avg, [cl_loss_avg, inst_loss_avg, ct_loss_avg], counts], {dataset_handle: train_handle, \n                                                                                               training:True})#, itr:tt})\n        sys.stdout.write('\\riter: {}, total_loss: {:.4f}, cl_loss: {:.4f}, inst_loss: {:.4f}, ct_loss: {:.4f}'.format(it, tl, *ls))\n        \n        if (it%steps_per_epoch) == 0:\n            print('Validation')\n            sess.run([step_reset, loss_avg_reset, cl_loss_avg_reset, inst_loss_avg_reset, ct_loss_avg_reset], \n                      {training:True, dataset_handle: train_handle})#, itr:tt})\n            for vt in range(1, num_valid_steps):\n                tl, ls = sess.run([loss_avg, [cl_loss_avg, inst_loss_avg, ct_loss_avg]], {dataset_handle: valid_handle, training:False})\n                                                                                     #itr:tt})\n                sys.stdout.write('\\rval iter: {}, total_loss: {:.4f}, cl_loss: {:.4f}, inst_loss: {:.4f}, ct_loss: {:.4f}'.format(vt, tl, *ls))\n            sess.run([step_reset, loss_avg_reset, cl_loss_avg_reset, inst_loss_avg_reset, ct_loss_avg_reset], \n                      {training:True, dataset_handle: train_handle})#, itr:tt})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47d7434dba1170de789501af13f56b2214c801bf"
      },
      "cell_type": "code",
      "source": "saver = tf.train.Saver()\nbest_dice = 0\nwith tf.Session() as sess:\n    if config.ckpt is not None:\n        print('Restoring weights from', config.ckpt)\n        saver.restore(sess, config.ckpt)\n    else:\n        sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    init_iterators(sess)\n    train_handle, valid_handle = get_itr_handles(sess)\n    \n    train_writer = tf.summary.FileWriter(\n        os.path.join(config.logs_path, 'train'), graph=sess.graph)\n    val_writer = tf.summary.FileWriter(\n        os.path.join(config.logs_path, 'val'), graph=sess.graph)\n    img_writer = tf.summary.FileWriter(\n        os.path.join(config.logs_path, 'img'), graph=sess.graph)\n    \n    tq_train = tqdm_notebook(range(config.iters_done+1, config.iters_done+config.n_iters+1), \n                             initial=config.iters_done+1)\n    \n    for it in tq_train:\n        fetch = [train_step, loss_avg, dice_avg, train_summary_op, img_summary_op]\n        fetch_vals = sess.run(fetch, {dataset_handle: train_handle, training:True})\n        _, loss_val, dice_val, train_sum_str, img_sum_str = fetch_vals\n        \n        tq_train.set_postfix(loss=loss_val, dice=dice_val)\n        train_writer.add_summary(train_sum_str, it)\n        img_writer.add_summary(img_sum_str, it)\n        \n        \n        if (it%config.valid_every) == 0:\n            sess.run([step_reset, loss_avg_reset, dice_avg_reset], {training:True, dataset_handle: train_handle})\n            tq_valid = tqdm_notebook(range(1, config.valid_iters+1), initial=1)\n            \n            for val_iter in tq_valid:\n                fetch_valid = [loss_avg, dice_avg, valid_summary_op, img_summary_op]\n                fetch_valid_vals = sess.run(fetch_valid, {dataset_handle: valid_handle, training:False})\n                loss_valid_val, dice_valid_val, val_sum_str, img_val_sum_str = fetch_valid_vals \n                img_writer.add_summary(img_sum_str, it)\n                \n                tq_valid.set_postfix(loss=loss_valid_val, dice=dice_valid_val)\n            \n            sess.run([step_reset, loss_avg_reset, dice_avg_reset], {training:False, dataset_handle: valid_handle})\n            val_writer.add_summary(val_sum_str, it)\n        \n        \n            present_dice = dice_valid_val #avg_dict_val['dice']\n            if present_dice > best_dice:\n                print('Dice increased from {:.4f} to {:.4f}'.format(best_dice, present_dice))\n                save_path = saver.save(sess=sess, \n                                       save_path='{}/{}_best'.format(config.save_path, config.model_name))\n                print('Saving to {}'.format(save_path))\n                best_dice = present_dice\n        \n    saver.save(sess=sess, \n               save_path='{}/{}_last'.format(config.save_path, config.model_name))\n        \n            \n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d12c04c1eed0b35f79aeac814e2b2844c144283b"
      },
      "cell_type": "code",
      "source": "config = Config()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "173f87ebcfc37f809586c9aaf8c3555352d65d8d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}