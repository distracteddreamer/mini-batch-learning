{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Dropout as a Bayesian Approximation \n",
    "\n",
    "The following my notes to accompany the appendix to \"Dropout as a Bayesian Approximation\". They are not guaranteed to be error-free. In fact they are very likely to contain errors. All errors are my own. In general the appendix explains the principles and proofs in a relatively detailed manner but not and again it makes jumps that I felt the need to bridge on my own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The key idea is that any neural network in which dropout is applied before every weight layer - and with no other requirements regarding its architecture - approximates a Deep Gaussian process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3 Equation (7)\n",
    "\n",
    "We want to minimise the KL divergence between the conditional distribution $p(\\boldsymbol{\\omega}|\\mathbf{X}, \\mathbf{Y})$ and the variational distribution $q(\\boldsymbol{\\omega})$. \n",
    "\n",
    "$$KL(q(\\boldsymbol{\\omega})\\Vert p(\\boldsymbol{\\omega}|\\mathbf{X}, \\mathbf{Y}))$$\n",
    "\n",
    "It is then stated that minimising this is equivalent to maximising the log evidence lower bound as given in (7):\n",
    "\n",
    "$$\\mathcal{L}_{VI} = \\int q(\\boldsymbol{\\omega})p(\\mathbf{Y}|\\mathbf{X}, \\boldsymbol{\\omega}) d\\boldsymbol{\\omega} -  \\log KL(q(\\boldsymbol{\\omega})\\Vert p(\\boldsymbol{\\omega}|\\mathbf{X}, \\mathbf{Y}))$$\n",
    "\n",
    "The KL-divergence in the above is between the variational distribution and the *prior* $p(\\boldsymbol{\\omega})$ incontrast to the earlier KL-divergence. \n",
    "\n",
    "We will show that $KL(q(\\boldsymbol{\\omega})\\Vert p(\\boldsymbol{\\omega}|\\mathbf{X}, \\mathbf{Y}))$ equals the negative (7) upto a constant that does not depend on $q(\\boldsymbol{\\omega})$.\n",
    "\n",
    "\n",
    "First consider the conditional probability $p(\\boldsymbol{\\omega}|\\mathbf{X}$ and \n",
    "\n",
    "$$p(\\boldsymbol{\\omega}|\\mathbf{X}, \\mathbf{Y})p(\\mathbf{X}, \\mathbf{Y})\n",
    "=p(\\boldsymbol{\\omega}|\\mathbf{X}, \\mathbf{Y})p(\\mathbf{Y} | \\mathbf{X})p(\\mathbf{X})$$\n",
    "\n",
    "using Bayes rule\n",
    "\n",
    "$$=p(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\omega})p({\\omega}) = p(\\mathbf{Y}|\\mathbf{X}, \\boldsymbol{\\omega})p(\\mathbf{X}|\\boldsymbol{\\omega})p({\\omega})$$\n",
    "\n",
    "since $\\mathbf{X}$ denotes the data which does not depend on $\\boldsymbol{\\omega}$:\n",
    "\n",
    "$$= p(\\mathbf{Y}|\\mathbf{X}, \\boldsymbol{\\omega})p(\\mathbf{X})p({\\omega})\\\\\n",
    "\\implies p(\\boldsymbol{\\omega}|\\mathbf{X}, \\mathbf{Y}) = p(\\mathbf{Y}|\\mathbf{X}, \\boldsymbol{\\omega})p({\\omega})/p(\\mathbf{Y} | \\mathbf{X})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting the above in the KL-divergence:\n",
    "\n",
    "$$KL(q(\\boldsymbol{\\omega})\\Vert p(\\boldsymbol{\\omega}|\\mathbf{X}, \\mathbf{Y}))\n",
    "= \\int \\log\\left[\\frac{q(\\boldsymbol{\\omega})}{p(\\boldsymbol{\\omega})} \\frac{p(\\mathbf{Y} | \\mathbf{X})}{p(\\mathbf{Y}|\\mathbf{X}, \\boldsymbol{\\omega})}\\right]q(\\boldsymbol{\\omega}) d\\boldsymbol{\\omega}\n",
    "\\\\=D_{KL}(q(\\boldsymbol{\\omega})\\Vert p(\\boldsymbol{\\omega})) \n",
    "- \\int \\log p(\\mathbf{Y} | \\mathbf{X}, {\\omega})q(\\boldsymbol{\\omega}) d\\boldsymbol{\\omega} \n",
    "+ \\underbrace{\\log p(\\mathbf{Y} | \\mathbf{X})}_{\\text{not a function of $q(\\boldsymbol{\\omega})$}}\\underbrace{\\int q(\\boldsymbol{\\omega}) d\\boldsymbol{\\omega}}_{=1}$$\n",
    "\n",
    "Since this equals the negative of (7) upto a constant, minimising this with respect to $q(\\boldsymbol{\\omega})$ is equivalent to maximising (7). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A - approximation of term inside logarithm\n",
    "\n",
    "The proof includes an approximation of the following expression:\n",
    "\n",
    "$$\\sum_{j=1}^L p_i(2\\pi)^{K/2}\\lvert \\boldsymbol{\\Sigma}_j \\rvert^{-1/2}\n",
    "\\exp\\left\\{-\\frac{1}{2}\\lVert\n",
    "\\boldsymbol{\\mu}_j - \\boldsymbol{\\mu}_i \n",
    "- \\mathbf{L}_i\\boldsymbol{\\epsilon}_i\n",
    "\\rVert_{\\boldsymbol{\\Sigma}_j}^2\\right\\} $$\n",
    "\n",
    "where the Mahalanobis distance is \n",
    "\n",
    "$$\\lVert\\mathbf{x}\\rVert_{\\mathbf{S}}^2 \n",
    "= \\mathbf{x}^T\\mathbf{S}^{-1}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "At this point the proof makes a somewhat abrupt jump \n",
    "\n",
    "> Using the expectation of the generalised $\\chi^2$ distribution with $K$ degrees of freedom, we have that for $K >> 0$ there exists that $\\lVert \\boldsymbol{\\mu}_j - \\boldsymbol{\\mu}_i - \\mathbf{L}_i\\boldsymbol{\\epsilon}_i\n",
    "\\rVert_{\\boldsymbol{\\Sigma}_j}^2>> 0$ for i $\\ne j$ (since the elements of $\\boldsymbol{\\Sigma}_j$ do not depend on $K$)\n",
    "\n",
    "\n",
    "It turns out that the term $\\lVert\n",
    "\\boldsymbol{\\mu}_j - \\boldsymbol{\\mu}_i \n",
    "- \\mathbf{L}_i\\boldsymbol{\\epsilon}_i\n",
    "\\rVert_{\\boldsymbol{\\Sigma}_j}^2$ is distributed according to a $\\chi^2$ distribution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let us briefly list a couple of the properties of a $\\chi^2$ random variable with $4$ degrees of freedom $Q \\sim $ we will need later:\n",
    "\n",
    "- It is the sum of the square of $r$ independent standard normal r.v.'s (hence $r$ degrees of freedom):\n",
    "\n",
    "$$Q \\sim \\chi^2(r)$$\n",
    "\n",
    "$$Q = \\sum_{i=1}^{r}z_i$$\n",
    "\n",
    "$$z_i \\sim \\mathcal{N}(0,1)$$\n",
    "\n",
    "- Its expected value is its number of degrees of freedom i.e. $E[\\chi^2] = r$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{x}_{ij} = \\boldsymbol{\\mu}_j - \\boldsymbol{\\mu}_i \\mathbf{L}_i\\boldsymbol{\\epsilon}_i$. We want to show that $\\lVert \\mathbf{x}_{ij} \\rVert_{\\boldsymbol{\\Sigma}_j}^2\\ \\sim \\chi^2(K)$. Note that $\\mathbf{L}_i$ is defined with respect to $\\boldsymbol{\\Sigma}_j$ such that $\\mathbf{L}_i\\mathbf{L}_i^T = \\boldsymbol{\\Sigma}_j$. Since $\\boldsymbol{\\Sigma}_j$ does not depend on $K$ this means that the expectation of $\\lVert \\mathbf{x}_{ij} \\rVert_{\\boldsymbol{\\Sigma}_j}^2$ depends only on the distribution of the term $\\mathbf{x}_{ij}$. As $\\mathbf{x}_{ij}$ has a normal distribution $\\mathcal{N}(\\mathbf{0}, \\Sigma_j)$, it will have the same expectation as a linear transformation of standard normal variable $\\mathbf{z}_{ij} = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_K)$ specifically $\\mathbf{x}_{ij} = \\mathbf{L}_i\\mathbf{z}_{ij}$.\n",
    "\n",
    "Using this we can express the Mahalanobis distance as follows: $$\\lVert\\mathbf{x}_{ij}\\rVert_{\\boldsymbol{\\Sigma}_j}^2 = \\mathbf{x}_{ij}^T\\mathbf{\\boldsymbol{\\Sigma}_j}^{-1}\\mathbf{x}_{ij}\n",
    "= (\\mathbf{L}_i\\mathbf{z}_{ij})^T\\mathbf(\\mathbf{L}_i \\mathbf{L}_i^T)^{-1}\\mathbf{L}_i\\mathbf{z}_{ij}\n",
    "= \\mathbf{z}_{ij}^T\\mathbf{L}_i^T\\mathbf(\\mathbf{L}_i^T)^{-1} \\mathbf{L}_i^{-1} \\mathbf{L}_i\\mathbf{z}_{ij}\n",
    "= \\mathbf{z}_{ij}^T\\mathbf{z}_{ij} = \\sum_{n=1}^{K} z_{ijn}^2$$\n",
    "\n",
    "where $z_{ijn}$ are the elements of $\\mathbf{z}_{ij}$\n",
    "\n",
    "Since the variance of $\\mathbf{z}_{ij}$ is $\\mathbf{I}_K$, each of $z_{ijn} \\sim \\mathcal{N}(0, 1)$ are independent standard normal variables. Therefore $$\\lVert\\boldsymbol{\\mu}_j - \\boldsymbol{\\mu}_i \\mathbf{L}_i\\boldsymbol{\\epsilon}_i\\rVert_{\\boldsymbol{\\Sigma}_j}^2  =  \\sum_{n=1}^{K} z_{ijn}^2 \\sim \\chi^2(K)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure what exactly they mean by \"we have that for $K >> 0$ there exists that $\\lVert \\boldsymbol{\\mu}_j - \\boldsymbol{\\mu}_i - \\mathbf{L}_i\\boldsymbol{\\epsilon}_i\n",
    "\\rVert_{\\boldsymbol{\\Sigma}_j}^2>> 0$\" but it looks like the approximation is based on approximating $\\lVert\\boldsymbol{\\mu}_j - \\boldsymbol{\\mu}_i - \\mathbf{L}_i\\boldsymbol{\\epsilon}_i\\rVert_{\\boldsymbol{\\Sigma}_j}^2$ by its expectation which is the number of degrees of freedom $K$ or on the similar basis that if the expectation is large then the value of the random variable is likely to be large. Since for large positive $a$, $\\exp(-a/2) \\longrightarrow 0$, the terms of the sum where $i\\ne j$ can be dropped.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
