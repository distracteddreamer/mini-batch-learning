{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from cleverhans.model_zoo.soft_nearest_neighbor_loss.SNNL_regularized_model import ModelBasicCNN\n",
    "from cleverhans.loss import SNNLCrossEntropy\n",
    "from data_utils import Config, DataPipeline\n",
    "from summary_utils import *\n",
    "from graph_utils import *\n",
    "from collections import namedtuple\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    data = tf.keras.datasets.mnist.load_data()\n",
    "    data = [(x.astype('float32'), y.astype('int32')) for x,y in data]\n",
    "    (x_trainval, y_trainval), (x_test, y_test) = data\n",
    "    #y_trainval = tf.keras.utils.to_categorical(y_trainval)\n",
    "    #y_test = tf.keras.utils.to_categorical(y_test)\n",
    "    n_trainval = len(x_trainval)\n",
    "    inds_train, inds_val = np.split(np.random.permutation(n_trainval), [n_trainval - len(x_test)])\n",
    "    return {'train': (x_trainval[inds_train]/255, y_trainval[inds_train]), \n",
    "            'valid': (x_trainval[inds_val]/255, y_trainval[inds_val]), \n",
    "            'test': (x_test/255, y_test)}\n",
    "\n",
    "def rand_bool(prob=0.5):\n",
    "    return tf.greater(tf.random_uniform([]), prob)\n",
    "\n",
    "def simple_aug(img, aug_prob):\n",
    "    img = img[None,...,None]\n",
    "    img = tf.cond(rand_bool(), \n",
    "                  lambda: tf.reverse(img, axis=tf.random_uniform([1], 1, 3, dtype=tf.int32)), \n",
    "                  lambda:img)\n",
    "    img = tf.cond(rand_bool(), \n",
    "                  lambda: tf.contrib.image.rotate(img, tf.random_uniform([], -1/6, 1/6)*np.pi), \n",
    "                  lambda: img)\n",
    "    img = tf.cond(rand_bool(),\n",
    "                  lambda: tf.contrib.image.translate(img, tf.random_uniform([2], 0, 4)),\n",
    "                  lambda: img)\n",
    "    return img[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp_masked(x, mask, axis):\n",
    "    x_max = tf.reduce_max(x, axis=axis, keepdims=True)\n",
    "    return x_max + tf.log(tf.reduce_sum(tf.exp(x - x_max)*mask, axis=axis))\n",
    "\n",
    "def get_snnl_loss(features, labels, temp=100.):\n",
    "    features = tf.layers.flatten(features)\n",
    "    x_not_equal = 1 - tf.eye(tf.shape(features)[0])\n",
    "    y_equal = tf.to_float(tf.equal(labels[:,None], labels[None]))\n",
    "    diff = -tf.reduce_sum((features[:,None] - features[None])**2,  axis=-1)/temp\n",
    "    eps = 0.00001 \n",
    "    exp_mat = tf.exp(diff)*x_not_equal\n",
    "    snnl_losses = tf.log(eps + tf.reduce_sum(exp_mat*y_equal, axis=-1)) - tf.log(eps + tf.reduce_sum(exp_mat, axis=-1))\n",
    "    return -tf.reduce_mean(snnl_losses)\n",
    "\n",
    "def ce_snnl_loss(logits, layers, labels, temp=100., factor=-10.):\n",
    "    ce_loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n",
    "    snnl_loss = tf.reduce_sum([get_snnl_loss(tf.layers.flatten(layer), labels, temp) for layer in layers])\n",
    "    return ce_loss + factor*snnl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "#[] Get MNIST inputs\n",
    "#[] Create pipeline from them\n",
    "tf.reset_default_graph()\n",
    "import numpy as np\n",
    "\n",
    "data = load_mnist()\n",
    "data['train'] = (np.concatenate([data['train'][0], data['valid'][0]]), \n",
    "                 np.concatenate([data['train'][1], data['valid'][1]]))\n",
    "data['valid'] = data['test']\n",
    "pipeline = DataPipeline(batch_size=128, \n",
    "                        map_fn=lambda x, y, mode: ((simple_aug(x, 0.5) if mode=='train' else x[...,None]), y))\n",
    "(images, labels), dataset_handle = pipeline.preproc(data)\n",
    "\n",
    "model = ModelBasicCNN(nb_classes=10, nb_filters=64, scope='')\n",
    "outputs = model.fprop(images)\n",
    "layers = [outputs['conv%i'%i] for i in range(1,4)]\n",
    "logits = outputs['logits']\n",
    "training = tf.placeholder(shape=[], dtype=tf.bool)\n",
    "\n",
    "loss = ce_snnl_loss(logits, layers, labels)\n",
    "ce_loss_op, weighted_snnl_loss_op = loss.op.inputs\n",
    "_, snnl_loss_op = weighted_snnl_loss_op.op.inputs\n",
    "\n",
    "labels_pred = tf.to_int32(tf.argmax(logits, axis=-1))\n",
    "acc = tf.reduce_mean(tf.to_float(tf.equal(labels_pred, labels)))\n",
    "\n",
    "losses = {'loss':loss, 'ce_loss':ce_loss_op, 'snnl_loss':snnl_loss_op}\n",
    "losses['acc'] = acc\n",
    "losses_avg, resets = add_metric_avg_ops(losses)\n",
    "train_summary_op = add_scalar_summaries(losses)\n",
    "valid_summary_op = add_scalar_summaries(losses_avg)\n",
    "train_step = get_train_op(loss, 'AdamOptimizer', learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4512acb11f4ea19babe8aec26e251e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = Config('test')\n",
    "config.set_train_stats(batch_size=128, \n",
    "                       num_train=len(data['train'][0]), \n",
    "                       num_val=len(data['valid'][0]),\n",
    "                       n_epochs=10)\n",
    "config.set_metric_attrs()\n",
    "config.make_paths()\n",
    "saver = tf.train.Saver()\n",
    "best_loss = config.best_metric \n",
    "with tf.Session() as sess:\n",
    "    if config.ckpt is not None:\n",
    "        print('Restoring weights from', config.ckpt)\n",
    "        saver.restore(sess, config.ckpt)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    pipeline.prepare(sess)\n",
    "    \n",
    "    train_handle, valid_handle = list(map(pipeline.handles.get, ['train', 'valid']))\n",
    "    train_writer, val_writer = get_summary_writers(sess, config.logs_path, ['train', 'valid'])\n",
    "    \n",
    "    tq_train = tqdm_notebook(range(config.iters_done+1, config.iters_done+config.n_iters+1), \n",
    "                             initial=config.iters_done+1)\n",
    "    \n",
    "    \n",
    "    for it in tq_train:\n",
    "        fetch = [train_step, losses_avg, train_summary_op]\n",
    "        fetch_vals = sess.run(fetch, {dataset_handle: train_handle, training:True})\n",
    "        _, losses_avg_val, train_sum_str = fetch_vals\n",
    "        \n",
    "        tq_train.set_postfix(**losses_avg_val)\n",
    "        train_writer.add_summary(train_sum_str, it)\n",
    "        \n",
    "        if (it%config.valid_every) == 0:\n",
    "            sess.run(resets, {training:True, dataset_handle: train_handle})\n",
    "            tq_valid = tqdm_notebook(range(1, config.valid_iters+1), initial=1)\n",
    "            \n",
    "            for val_iter in tq_valid:\n",
    "                fetch_valid = [losses_avg, valid_summary_op]\n",
    "                fetch_valid_vals = sess.run(fetch_valid, {dataset_handle: valid_handle, training:False})\n",
    "                losses_valid_val, val_sum_str = fetch_valid_vals\n",
    "                \n",
    "                tq_valid.set_postfix(**losses_valid_val)\n",
    "            \n",
    "            sess.run(resets, {dataset_handle: valid_handle, training:False})\n",
    "            val_writer.add_summary(val_sum_str, it)\n",
    "        \n",
    "        \n",
    "            present_loss = losses_valid_val['acc'] \n",
    "            if config.metric_compare(present_loss, best_loss):\n",
    "                print('Validation accuracy increased from {:.4f} to {:.4f}'.format(best_loss, \n",
    "                                                                               present_loss))\n",
    "                save_path = saver.save(sess=sess, \n",
    "                                       save_path='{}/best'.format(config.save_path))\n",
    "                print('Saving to {}'.format(save_path))\n",
    "                best_loss = present_loss\n",
    "                config.update_best(float(best_loss), int(it))\n",
    "                \n",
    "            config.iters_done = int(it)\n",
    "            config.save_json()\n",
    "            \n",
    "    saver.save(sess=sess, \n",
    "               save_path='{}/last'.format(config.save_path))\n",
    "\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
